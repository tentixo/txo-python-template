# utils/load_n_save.py
"""
Enhanced data handler with specialized methods and type safety.

Provides efficient file I/O with:
- Thread-safe lazy loading of heavy dependencies
- Type-safe category usage with validation
- Specialized methods for each file type
- Smart dispatcher for automatic routing
- Format detection and validation
- Comprehensive error handling
"""

import json
import gzip
import threading
from decimal import Decimal
from pathlib import Path
from typing import Union, Dict, Any, Optional, List, TYPE_CHECKING, Literal

from utils.logger import setup_logger
from utils.path_helpers import CategoryType, get_path, format_size
from utils.exceptions import FileOperationError, ValidationError

# Type checking imports (not loaded at runtime)
if TYPE_CHECKING:
    import pandas as pd
    import yaml

logger = setup_logger()

# File format detection types
FileFormat = Literal['json', 'text', 'csv', 'excel', 'yaml', 'binary', 'gzip', 'unknown']


class DecimalEncoder(json.JSONEncoder):
    """
    Custom JSON encoder that handles Decimal objects.

    Converts Decimal to float for JSON serialization.
    """

    def default(self, obj):
        """Convert Decimal to float, pass others to the default encoder."""
        if isinstance(obj, Decimal):
            return float(obj)
        return super().default(obj)


class TxoDataHandler:
    """
    Data I/O handler with specialized methods and type safety.

    Features:
    - Thread-safe lazy loading of heavy dependencies
    - Type-safe category usage
    - Specialized save/load methods for each format
    - Smart routing with format detection
    - Comprehensive error handling with fail-fast approach
    """

    # Class-level module cache with thread safety
    _modules: Dict[str, Any] = {}
    _import_lock = threading.Lock()

    # Configuration constants
    DEFAULT_ENCODING = 'utf-8'
    DEFAULT_JSON_INDENT = 2
    DEFAULT_COMPRESSION_LEVEL = 9
    DEFAULT_CSV_DELIMITER = ','
    DEFAULT_SHEET_NAME = 'Sheet1'

    # Format detection mappings
    FORMAT_EXTENSIONS = {
        '.json': 'json',
        '.txt': 'text', '.log': 'text', '.md': 'text',
        '.html': 'text', '.xml': 'text', '.py': 'text',
        '.csv': 'csv', '.tsv': 'csv',
        '.xlsx': 'excel', '.xls': 'excel', '.xlsm': 'excel',
        '.yaml': 'yaml', '.yml': 'yaml',
        '.gz': 'gzip', '.rapidstart': 'gzip',
        '.bin': 'binary', '.dat': 'binary', '.pkl': 'binary'
    }

    # ==================== Module Management ====================

    @classmethod
    def _lazy_import(cls, module_name: str) -> Any:
        """
        Thread-safe lazy import of modules.

        Args:
            module_name: Name of module to import ('pandas', 'yaml', 'openpyxl')

        Returns:
            Imported module

        Raises:
            ImportError: If module is not installed
            ValueError: If module name is not recognized
        """
        if module_name not in cls._modules:
            with cls._import_lock:
                # Double-check pattern for thread safety
                if module_name not in cls._modules:
                    logger.debug(f"Lazy loading {module_name} module")
                    try:
                        if module_name == 'pandas':
                            import pandas
                            cls._modules['pandas'] = pandas
                        elif module_name == 'yaml':
                            import yaml
                            cls._modules['yaml'] = yaml
                        elif module_name == 'openpyxl':
                            import openpyxl
                            cls._modules['openpyxl'] = openpyxl
                        else:
                            raise ValueError(f"Unknown module for lazy loading: {module_name}")
                    except ImportError as e:
                        error_msg = f"{module_name} not installed. Install with: pip install {module_name}"
                        logger.error(error_msg)
                        raise ImportError(error_msg) from e

        return cls._modules[module_name]

    @classmethod
    def clear_module_cache(cls) -> None:
        """
        Clear cached lazy-loaded modules to free memory.

        Useful for long-running processes or memory-constrained environments.
        """
        with cls._import_lock:
            count = len(cls._modules)
            cls._modules.clear()
            if count > 0:
                logger.debug(f"Cleared {count} cached modules from memory")

    # ==================== Format Detection ====================

    @staticmethod
    def detect_format(filename: str) -> FileFormat:
        """
        Detect file format from extension.

        Args:
            filename: Filename with extension

        Returns:
            Detected FileFormat
        """
        ext = Path(filename).suffix.lower()
        return TxoDataHandler.FORMAT_EXTENSIONS.get(ext, 'unknown')

    @staticmethod
    def validate_format(data: Any, filename: str, strict: bool = True) -> bool:
        """
        Validate that data type matches file extension.

        Args:
            data: Data to validate
            filename: Target filename
            strict: If True, raise exception on mismatch

        Returns:
            True if valid, False if not (when strict=False)

        Raises:
            ValidationError: If validation fails and strict=True
        """
        detected = TxoDataHandler.detect_format(filename)
        data_type = type(data).__name__

        valid = True
        error_msg = None

        if isinstance(data, str):
            if detected not in ('text', 'json', 'yaml'):
                valid = False
                error_msg = f"{data_type} data cannot be saved to {detected} format"
        elif isinstance(data, bytes):
            if detected not in ('binary', 'gzip'):
                valid = False
                error_msg = f"{data_type} data cannot be saved to {detected} format"
        elif isinstance(data, (dict, list)):
            if detected not in ('json', 'yaml'):
                valid = False
                error_msg = f"{data_type} data should be saved as json/yaml, not {detected}"
        elif hasattr(data, 'to_csv') and hasattr(data, 'to_excel'):
            if detected not in ('csv', 'excel'):
                valid = False
                error_msg = f"{data_type} should be saved as csv/excel, not {detected}"
        elif detected == 'unknown':
            valid = False
            error_msg = f"Unknown file format for extension: {Path(filename).suffix} (data type: {data_type})"

        if not valid:
            if strict:
                raise ValidationError(
                    f"Format validation failed: {error_msg}",
                    field='filename',
                    value=filename
                )
            else:
                logger.warning(f"Format validation: {error_msg}")

        return valid

    @staticmethod
    def suggest_extension(data: Any) -> str:
        """
        Suggest appropriate file extension based on data type.

        Args:
            data: Data to analyze

        Returns:
            Suggested extension with dot (e.g., '.json')
        """
        if isinstance(data, str):
            return '.txt'
        elif isinstance(data, bytes):
            return '.bin'
        elif isinstance(data, (dict, list)):
            return '.json'
        elif hasattr(data, 'to_csv'):
            return '.csv'
        elif hasattr(data, 'save'):
            return '.xlsx'
        return '.dat'

    # ==================== Main Load Dispatcher ====================

    @staticmethod
    def load(directory: CategoryType, filename: str, **kwargs) -> Any:
        """
        Smart load method that auto-detects format and routes to specialized loader.

        Args:
            directory: Target directory (use Categories.*)
            filename: Filename to load
            **kwargs: Format-specific options passed to specialized loader

        Returns:
            Loaded data in appropriate format

        Raises:
            FileNotFoundError: If file doesn't exist
            ValidationError: If file format is unknown
            Various format-specific exceptions
        """
        # Validate file exists
        file_path = get_path(directory, filename, ensure_parent=False)
        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {filename} in {directory}")

        # Detect format and route
        file_format = TxoDataHandler.detect_format(filename)

        if file_format == 'json':
            return TxoDataHandler.load_json(directory, filename)
        elif file_format == 'text':
            return TxoDataHandler.load_text(directory, filename, **kwargs)
        elif file_format == 'csv':
            return TxoDataHandler.load_csv(directory, filename, **kwargs)
        elif file_format == 'excel':
            return TxoDataHandler.load_excel(directory, filename, **kwargs)
        elif file_format == 'yaml':
            return TxoDataHandler.load_yaml(directory, filename)
        elif file_format == 'gzip':
            return TxoDataHandler.load_gzip(directory, filename)
        elif file_format == 'binary':
            return TxoDataHandler.load_binary(directory, filename)
        else:
            raise ValidationError(
                f"Unknown file format for {filename}. "
                f"Extension '{Path(filename).suffix}' not recognized",
                field='filename'
            )

    # ==================== Main Save Dispatcher ====================

    @staticmethod
    def save(data: Any, directory: CategoryType, filename: str, **kwargs) -> Path:
        """
        Smart save method that auto-routes to specialized saver based on data type.

        Args:
            data: Data to save (auto-detects type)
            directory: Target directory (use Categories.*)
            filename: Target filename with extension
            **kwargs: Format-specific options passed to specialized saver

        Returns:
            Path to saved file

        Raises:
            TypeError: Unsupported data type
            ValidationError: Data type doesn't match extension
        """
        # Validate format matches data
        TxoDataHandler.validate_format(data, filename, strict=True)

        # Route to specialized method based on data type
        if isinstance(data, str):
            return TxoDataHandler.save_text(data, directory, filename, **kwargs)
        elif isinstance(data, bytes):
            format_type = TxoDataHandler.detect_format(filename)
            if format_type == 'gzip':
                return TxoDataHandler.save_gzip(data, directory, filename, **kwargs)
            else:
                return TxoDataHandler.save_binary(data, directory, filename)
        elif isinstance(data, (dict, list)):
            format_type = TxoDataHandler.detect_format(filename)
            if format_type == 'yaml':
                return TxoDataHandler.save_yaml(data, directory, filename, **kwargs)
            else:
                return TxoDataHandler.save_json(data, directory, filename, **kwargs)
        elif hasattr(data, 'to_csv') and hasattr(data, 'to_excel'):
            return TxoDataHandler._save_dataframe(data, directory, filename, **kwargs)
        elif hasattr(data, 'save') and callable(data.save):
            return TxoDataHandler._save_workbook(data, directory, filename)
        else:
            raise TypeError(
                f"Unsupported data type: {type(data).__name__}. "
                f"Supported types: str, bytes, dict, list, DataFrame, Workbook. "
                f"Suggested extension: {TxoDataHandler.suggest_extension(data)}"
            )

    # ==================== Specialized Load Methods ====================

    @staticmethod
    def load_json(directory: CategoryType, filename: str) -> Union[Dict[str, Any], list]:
        """
        Load JSON file with proper error handling.

        Args:
            directory: Source directory (use Categories.*)
            filename: JSON filename

        Returns:
            Parsed JSON data (dict or list)

        Raises:
            FileOperationError: If file cannot be read
            ValidationError: If JSON is invalid
        """
        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading JSON from {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                logger.info(f"Loaded JSON from {file_path} ({file_path.stat().st_size:,} bytes)")
                return data
        except FileNotFoundError:
            raise FileOperationError(
                f"JSON file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except json.JSONDecodeError as e:
            raise ValidationError(
                f"Invalid JSON in {filename}: {e}",
                field='json_content'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to read JSON file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    @staticmethod
    def load_text(directory: CategoryType, filename: str,
                  encoding: Optional[str] = None) -> str:
        """
        Load text file with specified encoding.

        Args:
            directory: Source directory (use Categories.*)
            filename: Text filename
            encoding: Text encoding (default: UTF-8)

        Returns:
            File content as string

        Raises:
            FileOperationError: If file cannot be read
            UnicodeDecodeError: If encoding is incorrect
        """
        encoding = encoding or TxoDataHandler.DEFAULT_ENCODING
        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading text file from {file_path} with {encoding} encoding")

        try:
            content = file_path.read_text(encoding=encoding)
            logger.info(f"Loaded text from {file_path} ({len(content):,} chars)")
            return content
        except FileNotFoundError:
            raise FileOperationError(
                f"Text file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except UnicodeDecodeError as e:
            raise UnicodeDecodeError(
                e.encoding, e.object, e.start, e.end,
                f"Cannot decode {filename} with {encoding} encoding. Try different encoding."
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to read text file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    @staticmethod
    def load_yaml(directory: CategoryType, filename: str) -> Union[Dict[str, Any], list, Any]:
        """
        Load YAML file with lazy import.

        Args:
            directory: Source directory (use Categories.*)
            filename: YAML filename

        Returns:
            Parsed YAML data

        Raises:
            FileOperationError: If file cannot be read
            ValidationError: If YAML is invalid
        """
        yaml = TxoDataHandler._lazy_import('yaml')
        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading YAML from {file_path}")

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                logger.info(f"Loaded YAML from {file_path}")
                return data
        except FileNotFoundError:
            raise FileOperationError(
                f"YAML file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except yaml.YAMLError as e:
            raise ValidationError(
                f"Invalid YAML in {filename}: {e}",
                field='yaml_content'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to read YAML file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    @staticmethod
    def load_csv(directory: CategoryType, filename: str,
                 delimiter: Optional[str] = None,
                 encoding: Optional[str] = None,
                 usecols: Optional[List[str]] = None,
                 nrows: Optional[int] = None,
                 chunksize: Optional[int] = None) -> Union['pd.DataFrame', Any]:
        """
        Load CSV file with memory-efficient options.

        Args:
            directory: Source directory (use Categories.*)
            filename: CSV filename
            delimiter: Column delimiter (default: comma)
            encoding: File encoding (default: UTF-8)
            usecols: Columns to load (for memory efficiency)
            nrows: Number of rows to read
            chunksize: Return iterator for processing large files in chunks

        Returns:
            DataFrame or iterator if chunksize is specified

        Raises:
            FileOperationError: If file cannot be read
        """
        pd = TxoDataHandler._lazy_import('pandas')

        delimiter = delimiter or TxoDataHandler.DEFAULT_CSV_DELIMITER
        encoding = encoding or TxoDataHandler.DEFAULT_ENCODING

        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading CSV from {file_path}")

        try:
            df = pd.read_csv(
                file_path,
                encoding=encoding,
                delimiter=delimiter,
                usecols=usecols,
                nrows=nrows,
                chunksize=chunksize
            )

            if chunksize:
                logger.info(f"Loading CSV {file_path} in chunks of {chunksize}")
            else:
                logger.info(f"Loaded CSV from {file_path} ({len(df):,} rows)")

            return df

        except FileNotFoundError:
            raise FileOperationError(
                f"CSV file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except (OSError, IOError, pd.errors.ParserError) as e:
            raise FileOperationError(
                f"Failed to load CSV file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    @staticmethod
    def load_excel(directory: CategoryType, filename: str,
                   sheet_name: Union[str, int] = 0,
                   skiprows: int = 0,
                   usecols: Optional[List[str]] = None,
                   nrows: Optional[int] = None) -> 'pd.DataFrame':
        """
        Load Excel file with memory-efficient options.

        Args:
            directory: Source directory (use Categories.*)
            filename: Excel filename
            sheet_name: Sheet to load (name or index, default: 0)
            skiprows: Rows to skip from beginning
            usecols: Columns to load (for memory efficiency)
            nrows: Number of rows to read

        Returns:
            DataFrame with Excel data

        Raises:
            FileOperationError: If file cannot be read
            ValidationError: If sheet doesn't exist
        """
        pd = TxoDataHandler._lazy_import('pandas')
        _ = TxoDataHandler._lazy_import('openpyxl')  # Ensure openpyxl is available

        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading Excel from {file_path} (sheet: {sheet_name})")

        try:
            df = pd.read_excel(
                file_path,
                sheet_name=sheet_name,
                skiprows=skiprows,
                usecols=usecols,
                nrows=nrows,
                engine='openpyxl'
            )
            logger.info(f"Loaded Excel from {file_path} ({len(df):,} rows)")
            return df

        except FileNotFoundError:
            raise FileOperationError(
                f"Excel file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except ValueError as e:
            if 'Worksheet' in str(e):
                raise ValidationError(
                    f"Sheet '{sheet_name}' not found in {filename}",
                    field='sheet_name',
                    value=sheet_name
                )
            raise FileOperationError(
                f"Failed to load Excel file: {e}",
                file_path=str(file_path),
                operation='load'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to load Excel file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    @staticmethod
    def load_binary(directory: CategoryType, filename: str) -> bytes:
        """
        Load binary file.

        Args:
            directory: Source directory (use Categories.*)
            filename: Binary filename

        Returns:
            File content as bytes

        Raises:
            FileOperationError: If file cannot be read
        """
        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading binary file from {file_path}")

        try:
            content = file_path.read_bytes()
            logger.info(f"Loaded binary from {file_path} ({len(content):,} bytes)")
            return content
        except FileNotFoundError:
            raise FileOperationError(
                f"Binary file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to read binary file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    @staticmethod
    def load_gzip(directory: CategoryType, filename: str) -> bytes:
        """
        Load and decompress GZip file.

        Args:
            directory: Source directory (use Categories.*)
            filename: GZip filename

        Returns:
            Decompressed content as bytes

        Raises:
            FileOperationError: If file cannot be read or decompressed
        """
        file_path = get_path(directory, filename, ensure_parent=False)
        logger.debug(f"Loading GZip file from {file_path}")

        try:
            with gzip.open(file_path, 'rb') as gz_file:
                content = gz_file.read()
                logger.info(f"Loaded and decompressed {file_path} ({len(content):,} bytes)")
                return content
        except FileNotFoundError:
            raise FileOperationError(
                f"GZip file not found: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except gzip.BadGzipFile:
            raise FileOperationError(
                f"Invalid GZip file: {filename}",
                file_path=str(file_path),
                operation='load'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to load GZip file: {e}",
                file_path=str(file_path),
                operation='load'
            )

    # ==================== Specialized Save Methods ====================

    @staticmethod
    def save_json(data: Union[Dict, list],
                  directory: CategoryType,
                  filename: str,
                  compact: bool = False,
                  sort_keys: bool = False,
                  ensure_ascii: bool = False) -> Path:
        """
        Save data as JSON with formatting options.

        Args:
            data: JSON-serializable data (dict or list)
            directory: Target directory (use Categories.*)
            filename: Target filename
            compact: Minimize file size (no indentation)
            sort_keys: Sort dictionary keys alphabetically
            ensure_ascii: Force ASCII encoding

        Returns:
            Path to saved file

        Raises:
            TypeError: If data is not dict or list
            ValidationError: If data cannot be serialized
            FileOperationError: If file cannot be written
        """
        if not isinstance(data, (dict, list)):
            raise TypeError(
                f"JSON data must be dict or list, got {type(data).__name__}"
            )

        file_path = get_path(directory, filename)

        try:
            if compact:
                content = json.dumps(
                    data,
                    cls=DecimalEncoder,
                    separators=(',', ':'),
                    ensure_ascii=ensure_ascii,
                    sort_keys=sort_keys
                )
            else:
                content = json.dumps(
                    data,
                    indent=TxoDataHandler.DEFAULT_JSON_INDENT,
                    cls=DecimalEncoder,
                    ensure_ascii=ensure_ascii,
                    sort_keys=sort_keys
                )

            file_path.write_text(content, encoding='utf-8')
            size_str = format_size(len(content.encode('utf-8')))
            logger.info(f"Saved JSON to {file_path} ({size_str})")
            return file_path

        except (TypeError, ValueError) as e:
            raise ValidationError(
                f"JSON serialization failed: {e}",
                field='data'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save JSON file: {e}",
                file_path=str(file_path),
                operation='save'
            )

    @staticmethod
    def save_text(content: str,
                  directory: CategoryType,
                  filename: str,
                  encoding: Optional[str] = None,
                  ensure_newline: bool = False,
                  line_ending: Optional[Literal['unix', 'windows']] = None) -> Path:
        """
        Save text content with encoding options.

        Args:
            content: Text content to save
            directory: Target directory (use Categories.*)
            filename: Target filename
            encoding: Text encoding (default: UTF-8)
            ensure_newline: Ensure file ends with newline
            line_ending: Force specific line endings

        Returns:
            Path to saved file

        Raises:
            TypeError: If content is not string
            FileOperationError: If file cannot be written
        """
        if not isinstance(content, str):
            raise TypeError(
                f"Content must be str, got {type(content).__name__}"
            )

        encoding = encoding or TxoDataHandler.DEFAULT_ENCODING

        # Handle line endings
        if line_ending == 'unix':
            content = content.replace('\r\n', '\n').replace('\r', '\n')
        elif line_ending == 'windows':
            content = content.replace('\r\n', '\n').replace('\r', '\n')
            content = content.replace('\n', '\r\n')

        # Ensure newline at end if requested
        if ensure_newline and content and not content.endswith('\n'):
            content += '\n'

        file_path = get_path(directory, filename)

        try:
            file_path.write_text(content, encoding=encoding)
            logger.info(f"Saved text to {file_path} ({len(content):,} chars, {encoding})")
            return file_path
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save text file: {e}",
                file_path=str(file_path),
                operation='save'
            )

    @staticmethod
    def save_yaml(data: Union[Dict, list, Any],
                  directory: CategoryType,
                  filename: str,
                  default_flow_style: bool = False,
                  sort_keys: bool = False) -> Path:
        """
        Save data as YAML file.

        Args:
            data: Data to save (usually dict or list)
            directory: Target directory (use Categories.*)
            filename: Target filename
            default_flow_style: Use flow style {a: 1} vs block style
            sort_keys: Sort dictionary keys

        Returns:
            Path to saved file

        Raises:
            ValidationError: If data cannot be serialized
            FileOperationError: If file cannot be written
        """
        yaml = TxoDataHandler._lazy_import('yaml')
        file_path = get_path(directory, filename)

        try:
            yaml_content = yaml.dump(
                data,
                default_flow_style=default_flow_style,
                sort_keys=sort_keys,
                allow_unicode=True,
                encoding=None  # Return string, not bytes
            )

            file_path.write_text(yaml_content, encoding='utf-8')
            logger.info(f"Saved YAML to {file_path}")
            return file_path

        except yaml.YAMLError as e:
            raise ValidationError(
                f"YAML serialization failed: {e}",
                field='data'
            )
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save YAML file: {e}",
                file_path=str(file_path),
                operation='save'
            )

    @staticmethod
    def save_binary(data: bytes,
                    directory: CategoryType,
                    filename: str) -> Path:
        """
        Save binary data to file.

        Args:
            data: Binary data to save
            directory: Target directory (use Categories.*)
            filename: Target filename

        Returns:
            Path to saved file

        Raises:
            TypeError: If data is not bytes
            FileOperationError: If file cannot be written
        """
        if not isinstance(data, bytes):
            raise TypeError(
                f"Data must be bytes for binary save, got {type(data).__name__}"
            )

        file_path = get_path(directory, filename)

        try:
            file_path.write_bytes(data)
            size_str = format_size(len(data))
            logger.info(f"Saved binary to {file_path} ({size_str})")
            return file_path
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save binary file: {e}",
                file_path=str(file_path),
                operation='save'
            )

    @staticmethod
    def save_gzip(data: Union[str, bytes],
                  directory: CategoryType,
                  filename: str,
                  compression_level: Optional[int] = None) -> Path:
        """
        Save data as GZip compressed file.

        Args:
            data: String or bytes to compress
            directory: Target directory (use Categories.*)
            filename: Target filename (should end with .gz)
            compression_level: Compression level 0-9 (9=max)

        Returns:
            Path to saved file

        Raises:
            FileOperationError: If file cannot be written
        """
        compression_level = compression_level or TxoDataHandler.DEFAULT_COMPRESSION_LEVEL

        if not 0 <= compression_level <= 9:
            raise ValueError(f"Compression level must be 0-9, got {compression_level}")

        file_path = get_path(directory, filename)

        try:
            # Convert string to bytes if needed
            content = data.encode('utf-8') if isinstance(data, str) else data
            original_size = len(content)

            with gzip.open(file_path, 'wb', compresslevel=compression_level) as gz_file:
                gz_file.write(content)

            # Log compression stats
            compressed_size = file_path.stat().st_size
            ratio = (1 - compressed_size / original_size) * 100 if original_size > 0 else 0
            logger.info(
                f"Saved GZip to {file_path}: "
                f"{format_size(original_size)} → {format_size(compressed_size)} "
                f"({ratio:.1f}% compression, level={compression_level})"
            )

            return file_path

        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save GZip file: {e}",
                file_path=str(file_path),
                operation='save'
            )

    # ==================== Private Helper Methods ====================

    @staticmethod
    def _save_dataframe(df: 'pd.DataFrame',
                        directory: CategoryType,
                        filename: str,
                        index: bool = False,
                        sheet_name: Optional[str] = None,
                        **kwargs) -> Path:
        """
        Internal method to save DataFrames.

        Args:
            df: DataFrame to save
            directory: Target directory
            filename: Target filename
            index: Include index in output
            sheet_name: Excel sheet name
            **kwargs: Additional format-specific options

        Returns:
            Path to saved file

        Raises:
            ValueError: If file extension is not supported
            FileOperationError: If save fails
        """
        file_path = get_path(directory, filename)
        file_ext = file_path.suffix.lower()

        try:
            if file_ext == '.csv':
                df.to_csv(file_path, index=index, **kwargs)
            elif file_ext in ('.xlsx', '.xls'):
                sheet_name = sheet_name or TxoDataHandler.DEFAULT_SHEET_NAME
                df.to_excel(
                    file_path,
                    index=index,
                    sheet_name=sheet_name,
                    engine='openpyxl',
                    **kwargs
                )
            else:
                raise ValueError(
                    f"Unsupported extension for DataFrame: {file_ext}. "
                    f"Use .csv or .xlsx"
                )

            logger.info(f"Saved DataFrame to {file_path} ({len(df):,} rows)")
            return file_path

        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save DataFrame: {e}",
                file_path=str(file_path),
                operation='save'
            )

    @staticmethod
    def _save_workbook(workbook: Any,
                       directory: CategoryType,
                       filename: str) -> Path:
        """
        Internal method to save openpyxl Workbooks.

        Args:
            workbook: Workbook object with save() method
            directory: Target directory
            filename: Target filename

        Returns:
            Path to saved file

        Raises:
            FileOperationError: If save fails
        """
        file_path = get_path(directory, filename)

        try:
            workbook.save(file_path)
            logger.info(f"Saved Workbook to {file_path}")
            return file_path
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to save Workbook: {e}",
                file_path=str(file_path),
                operation='save'
            )

    # ==================== Utility Methods ====================

    @staticmethod
    def exists(directory: CategoryType, filename: str,
               check_empty: bool = False) -> bool:
        """
        Check if a file exists and optionally if it's not empty.

        Args:
            directory: Directory to check (use Categories.*)
            filename: Filename to check
            check_empty: Also verify file is not empty

        Returns:
            True if file exists (and is not empty if check_empty=True)
        """
        file_path = get_path(directory, filename, ensure_parent=False)

        if not file_path.exists():
            return False

        if check_empty and file_path.stat().st_size == 0:
            logger.warning(f"File exists but is empty: {file_path}")
            return False

        return True

    @staticmethod
    def delete(directory: CategoryType, filename: str,
               safe: bool = True) -> bool:
        """
        Delete a file from the specified directory.

        Args:
            directory: Directory containing file (use Categories.*)
            filename: File to delete
            safe: If True, don't error if file doesn't exist

        Returns:
            True if file was deleted, False if it didn't exist

        Raises:
            FileNotFoundError: If file doesn't exist and safe=False
            FileOperationError: If deletion fails
        """
        file_path = get_path(directory, filename, ensure_parent=False)

        try:
            if file_path.exists():
                file_path.unlink()
                logger.info(f"Deleted file: {file_path}")
                return True
            elif not safe:
                raise FileNotFoundError(f"File not found: {file_path}")
            else:
                logger.debug(f"File already absent: {file_path}")
                return False
        except (OSError, IOError) as e:
            raise FileOperationError(
                f"Failed to delete file: {e}",
                file_path=str(file_path),
                operation='delete'
            )

    @staticmethod
    def get_size(directory: CategoryType, filename: str) -> int:
        """
        Get the size of a file in bytes.

        Args:
            directory: Directory containing file (use Categories.*)
            filename: Filename to check

        Returns:
            File size in bytes

        Raises:
            FileNotFoundError: If file doesn't exist
        """
        file_path = get_path(directory, filename, ensure_parent=False)

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        size = file_path.stat().st_size
        logger.debug(f"File {filename} size: {format_size(size)}")
        return size